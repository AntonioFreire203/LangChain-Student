{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mensagens com contexto (System, Human, AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso simula uma conversa com contexto:\n",
    "\n",
    "    SystemMessage: Define o papel do assistente.\n",
    "\n",
    "    HumanMessage: Entrada do usuário.\n",
    "\n",
    "    AIMessage: Resposta anterior do assistente.\n",
    "\n",
    "Usamos isso para manter uma linha de conversa contínua, o que melhora a coerência das respostas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente atencioso especializado em fornecer informações sobre a loja de material esportivo Centauro .\"),\n",
    "    HumanMessage(content=\"O que tem na loja?\"),\n",
    "    AIMessage(content=\"O Centauro oferece uma variedade de produtos desde material de esportes aquaticos a meterial de corrida \"),\n",
    "    HumanMessage(content=\"Vocês têm camisas termicas da nike?\")\n",
    "]\n",
    "\n",
    "llm_result = llm.invoke(input=messages)\n",
    "print(llm_result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento em lote para Chat Models                                                                                                                                                                                                                                                                                                                    \n",
    "\n",
    " Aqui enviamos duas conversas diferentes ao mesmo tempo, cada uma com um papel diferente \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"Você é um assistente útil que traduz frases do portugues-br para o japonês.\"),\n",
    "        HumanMessage(content=\"Você tem opções veganas?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"Você é um assistente útil que traduz frases do portugues-br para o espanhol.\"),\n",
    "        HumanMessage(content=\"Você tem opções veganas?\")\n",
    "    ],\n",
    "]\n",
    "\n",
    "batch_result = llm.generate(batch_messages)\n",
    "\n",
    "# Extraindo as traduções\n",
    "translations = [generation[0].text for generation in batch_result.generations]\n",
    "print(translations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
